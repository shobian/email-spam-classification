{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The curse of Dimensionality\n",
    "\n",
    "Humans are bound by their perception of a maximum of three dimensions. We can’t comprehend shapes/graphs beyond three dimensions. Often, data scientists get datasets which have thousands of features. They give birth to two kinds of problems:\n",
    "\n",
    "* **Increase in computation time:** Majority of the machine learning algorithms they rely on the calculation of distance for model building and as the number of dimensions increases it becomes more and more computation-intensive to create a model out of it. For example, if we have to calculate the distance between two points in just one dimension, like two points on the number line, we’ll just subtract the coordinate of one point from another and then take the magnitude:\n",
    "\n",
    "Distance= $ x_1-x_2 $\n",
    "\n",
    "What if we need to calculate the distance between two points in two dimensions?\n",
    "\n",
    "The same formula translates to:\n",
    "Distance= $ \\sqrt {(x_1-x_2)^2+(y_1-y_2)^2} $\n",
    "\n",
    "What if we need to calculate the distance between two points in three dimensions?\n",
    "\n",
    "The same formula translates to:\n",
    "Distance= $ \\sqrt {(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}$\n",
    "\n",
    "And for N-dimensions, the formula becomes:\n",
    "Distance=$ \\sqrt {(a_1-a_2)^2+(b_1-b_2)^2+(c_1-c_2)^2+…+(n_1-n_2)^2} $\n",
    "\n",
    "This is the effort of calculating the distance between two points. Just imagine the number of calculations involved for all the data points involved.\n",
    "\n",
    "One more point to consider is that as the number of dimension increases, points are going far away from each other. This means that any new point that comes when we are testing the model is going to be farther away from our training points. This leads to a less reliable model, and it makes our model overfitted to the training data.\n",
    "\n",
    "\n",
    "\n",
    "* **Hard (or almost impossible) to visualise the relationship between features:** As stated above, humans can not comprehend things beyond three dimensions. So, if we have an n-dimensional dataset, the only solution left to us is to create either a 2-D or 3-D graph out of it. Let’s say for simplicity, we are creating 2-D graphs. Suppose we have 1000 features in the dataset. That results in a  total (1000*999)/2= 499500 combinations possible for creating the 2-D graph.\n",
    "\n",
    "Is it humanly possible to analyse all those graphs to understand the relationship between the variables?\n",
    "\n",
    "**The questions that we need to ask at this point are:**\n",
    "\n",
    "* Are all the features really contributing to decision making?\n",
    "* Is there a way to come to the same conclusion using a lesser number of features?\n",
    "* Is there a way to combine features to create a new feature and drop the old ones?\n",
    "* Is there a way to remodel features in a way to make them visually comprehensible?\n",
    "\n",
    "The answer to all the above questions is- _Dimensionality Reduction technique._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Dimensionality Reduction Technique?\n",
    "\n",
    "Dimensionality reduction is a feature selection technique using which we reduce the number of features to be used for making a model without losing a significant amount of information compared to the original dataset. In other words, a dimensionality reduction technique projects a data of higher dimension to a lower-dimensional subspace.\n",
    "\n",
    "**When to use Dimensionality Reduction?**\n",
    "Dimensionality reduction shall be used before feeding the data to a machine learning algorithm to achieve the following:\n",
    "\n",
    "* It reduces the size of the space in which the distances are calculated, thereby improving machine learning algorithm performance.\n",
    "* It reduces the degrees of freedom for our dataset avoiding chances of overfitting.\n",
    "* Reducing the dimensionality using dimensionality reduction techniques can simplify the dataset facilitating a better description, visualisation, and insight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis: \n",
    "The principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n",
    "\n",
    "**What are the principal components?**\n",
    "Principal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features.\n",
    "\n",
    "Let's take the following example where the data is distributed like the diagram on the left:\n",
    "<img src=\"PCA_intro1.PNG\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above, we are considering 3 orthogonal(_C3  is in the third dimension_) axes to show the distribution of data. If you notice the diagram on the right, the first two axes **C1** and **C2** successfully explain the maximum variation in the data whereas the axes **C3** only consists of a fewer number of points. Hence, while considering the principal components C1 and C2 will be our choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics Behind PCA\n",
    "We are going to discuss PCA using a method called Singular Value Decomposition (SVD) which factorises the dataset matrix in such a way that it becomes a  product of the multiplication of three individual matrices:\n",
    "\n",
    "X(original Data)= $ U* \\Sigma* V^T$\n",
    "\n",
    "Where V is the matrix that contains the principal components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisite:** PCA assumes that the mean of all the individual columns is zero and the standard deviation is 1. So, before applying PCA, the data should be pre-processed appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a simple example to understand it: \n",
    "\n",
    "Let’s suppose we have the following dataset:\n",
    "\n",
    "<img src=\"table.PNG\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Calculate PCA\n",
    "* Let’s plot this on the XY plane and calculate the average of the magnitude of all the points. Blue ones are the actual points and the yellow one is the average point.\n",
    "\n",
    "<img src=\"points.PNG\" width=\"300\">\n",
    "\n",
    "* Move the points so that the average point is on the origin. This is called a parallel translation. Although the coordinates of the points have changed, the corresponding distances among them remain the same.\n",
    "\n",
    "<img src=\"move_average.PNG\" width=\"300\">\n",
    "\n",
    "* Create the best fit line for the new data points. We first start with a random line(blue one), and then try to find the best fit line(the green one) so that the distance from individual data points is minimum and consequently the distance from origin is maximum. This best fit line is called Principal component1 or PC1.\n",
    "\n",
    "<img src=\"best_fit.PNG\" width=\"300\">\n",
    "\n",
    "* PC2 is a line perpendicular to the PC1.\n",
    "* Then the axes PC1 and PC2 are rotated in a way that PC1 becomes the horizontal axis.\n",
    "<img src=\"axis_rotate.PNG\" width=\"300\">\n",
    "* Then based on the sample points the new points are projected using PC1 and PC2. Thus we get the derived features. \n",
    "<img src=\"projected_points.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the question is: if we talk about n dimensions, there are n-1 perpendicular lines possible on PC1. **How to select a line as PC2?**\n",
    "\n",
    "And the next question is: **what is the optimum number of Principal components needed?**\n",
    "\n",
    "The answer to these lie with explained variance ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Variance Ratio\n",
    "\n",
    "All of the above questions are answered using the *explained variance ratio*. It represents the amount of variance each principal component is able to explain.\n",
    "\n",
    "For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n",
    "\n",
    "EVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{50}{55}=0.91 $\n",
    "\n",
    "EVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{5}{55}=0.09 $\n",
    "\n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n",
    "\n",
    "In a real-life scenario, this problem is solved using the **Scree Plots**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree Plots:\n",
    "Scree plots are the graphs that convey how much variance is explained by corresponding Principal components. \n",
    "<img src=\"scree.PNG\" width=\"500\">\n",
    "\n",
    "As shown in the given diagram, around 75 principal components explain approximately 90 % of the variance. Hence, 75 can be a good choice based on the scenario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Maths involved through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Creating an Array\n",
    "A = np.array([\n",
    "        [ 3,  7],\n",
    "        [-4, -6],\n",
    "        [ 7,  8],\n",
    "        [ 1, -1],\n",
    "        [-4, -1],\n",
    "        [-3, -7]\n",
    "    ])\n",
    "\n",
    "m,n = A.shape # m-observations, n-features\n",
    "\n",
    "print(\"Array:\")\n",
    "print(A) # our array\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Dimensions:\")\n",
    "print(A.shape) # shape\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Mean across Rows:\")\n",
    "print(np.mean(A,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the array into a DataFrame ...\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(A, columns = ['a0', 'a1'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and a dataframe can as easily be converted to an array\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance\n",
    "\n",
    "_Variance_ is the measure of how a variable changes or varies and _co_ means together. Hence, _covariance_ is the measure of how two variables change together.\n",
    "<img src=\"covariance.PNG\" width=\"500\">\n",
    "\n",
    "If the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too.\n",
    "Generally, we avoid using highly correlated variables in building a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# makes charts pretty\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "plt.scatter(A[:,0],A[:,1])   # create a scatter plot \n",
    "\n",
    "# annotations\n",
    "for i in range(m):\n",
    "    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Dataset $A$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample covariance between $a_0$ and $a_1$:\n",
    "\n",
    "$$\n",
    "cov_{a_0,a_1} =\\frac{\\sum_{k=0}^{m-1}(a_0^k - \\bar{a_0})(a_1^k - \\bar{a_1})}{m-1}\n",
    "$$\n",
    "\n",
    "where $\\bar{a_0}$ is the mean of column $a_0$ and $\\bar{a_1}$ is the mean of column $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance of a0 and a1\n",
    "a0 = A[:,0]\n",
    "a1 = A[:,1]\n",
    "product = a0*a1 # element-wise product\n",
    "print(\"Length of prod equals \" + str(len(product)))\n",
    "print(\"---\")\n",
    "print(\"Covariance:\")\n",
    "print(np.sum(product)/(m-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get more stuff using NumPy's covariance method\n",
    "np.cov(a0,a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear Algebra way:\n",
    "$$\n",
    "\\Sigma = \\frac{A^TA}{(m-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is A.T?\n",
    "A.T # This is the transpose of matrix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Multiplication, @ operator is used for calculating the dot product of two matrices\n",
    "A.T @ A # or np.dot(A.T,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated in the formula now we need to divide the  product by (m-1) to yield true Sample Covariance Matrix\n",
    "# Let's call it Sigma\n",
    "Sigma = (A.T @ A)/(m-1) # or np.cov(A.T)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen-decomposition of $\\Sigma$\n",
    "\n",
    "According to [Wikipedia article on PCA](https://en.m.wikipedia.org/wiki/Principal_component_analysis), *\"PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix.\"* The second approach has already been discussed above. Let's discuss the first approach now.\n",
    "\n",
    "$\\Sigma$ is a real, symmetric matrix; thus, it has \n",
    "\n",
    "1) real eigenvalues, and\n",
    "\n",
    "2) orthogonal eigenvectors.\n",
    "\n",
    "Definition:\n",
    "\n",
    "An **eigenvector v** of a linear transformation **T** is a nonzero vector that, when **T** is applied to it, does not change direction. Applying __T__ to the eigenvector only scales the eigenvector by the scalar value λ, called an **eigenvalue**. This condition can be written as the equation\n",
    "\n",
    "$$\n",
    "{\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the eigenvalues and eigen vectors for the matrix Sigma\n",
    "l, X = np.linalg.eig(Sigma)\n",
    "print(\"Eigenvalues:\")\n",
    "print(l)\n",
    "print(\"---\")\n",
    "print(\"Eigenvectors:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from your Linear Algebra class that the following should hold:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Sigma x_0 &=& \\lambda_0 x_0 \\nonumber \\\\\n",
    "\\Sigma x_1 &=& \\lambda_1 x_1 \\nonumber \\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the first Eigenvalue, Eigenvector combination\n",
    "print(\"Sigma times eigenvector:\")\n",
    "print(Sigma @ X[:,0]) # 2x2 times 2x1\n",
    "print(\"Eigenvalue times eigenvector:\")\n",
    "print(l[0] * X[:,0]) # scalar times 2x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and the product with the second eigenvalue\n",
    "print(\"Sigma times eigenvector:\")\n",
    "print(Sigma @ X[:,1]) # 2x2 times 2x1\n",
    "print(\"Eigenvalue times eigenvector:\")\n",
    "print(l[1] * X[:,1]) # scalar times 2x1, ANNOYING - MUST USE * vs. @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The first principal component is eigenvector with largest evalue:\")\n",
    "print(X[:,1])\n",
    "print(\"---\")\n",
    "print(\"Second principal component:\")\n",
    "print(X[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the two Principal components Orthogonal? If the dot product of two matrices is zero, then they are considered to be orthogonal\n",
    "X[:,1].T @ X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the Eigen Vectors\n",
    "plt.scatter(A[:,0],A[:,1])\n",
    "scale = 3 # increase this scaling factor to highlight these vectors\n",
    "plt.plot([0,X[0,1]*scale],[0,X[1,1]*scale],'r') # First principal component\n",
    "plt.plot([0,X[0,0]*scale],[0,X[1,0]*scale],'g') # Second principal component\n",
    "\n",
    "# annotations\n",
    "for i in range(m):\n",
    "    plt.annotate('('+str(A[i,0])+','+str(A[i,1])+')',(A[i,0]+0.2,A[i,1]+0.2))\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Eigenvectors of $\\Sigma$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction: 2D to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to matrix\n",
    "Amat = np.asmatrix(A)\n",
    "Xmat = np.asmatrix(X)\n",
    "Amat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose eigenvector with highest eigenvalue as first principal component\n",
    "pc1 = Xmat[:,1]\n",
    "pc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acomp = Amat @ pc1 # the dot product of a 6x2 and 2x1 matrix yields a 6x1 matrix\n",
    "print(\"Compressed version of A:\")\n",
    "print(Acomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arec = Acomp @ pc1.T # the dot product of a  6x1 matrix and 1x2 matrix results into a 6x2 matrix\n",
    "print(\"Reconstruction from 1D compression of A:\")\n",
    "print(Arec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Reconstructing the 1D compression of $A$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.matrix_rank(Amat)) # originally a Rank 2 matrix\n",
    "print(np.linalg.matrix_rank(Arec)) # reconstructed matrix is Rank 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking on the Rank-1 matrix related to the 2nd eigenvector you get back to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Rank 1 matrix for the other vector to recover A completely\n",
    "# Here we are taking the dot product of matrix A with the principal components and the transpose of the principal components\n",
    "Amat @ Xmat[:,1] @ Xmat[:,1].T + Amat @ Xmat[:,0] @ Xmat[:,0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why does this work? Well, recall that the dot product of a matrix and its transpose (X @ X.T) is an identity matrix as X is orthonormal\n",
    "# Hence the entire expression becomes equivalent to multiplying a matrix with a unit matrix which returns the matrix itself.\n",
    "A @ Xmat @ Xmat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "plt.scatter(A[:,0], A[:,1]) # A in blue\n",
    "plt.plot(Arec[:,0],Arec[:,1],'r', marker='o') # Arec in RED\n",
    "\n",
    "# across observations\n",
    "for i in range(m):\n",
    "    e = np.vstack((A[i],Arec[i]))\n",
    "    plt.plot(e[:,0],e[:,1],'b') # BLUE\n",
    "\n",
    "# axes\n",
    "plt.plot([-6,8],[0,0],'grey') # x-axis\n",
    "plt.plot([0,0],[-8,10],'grey') # y-axis\n",
    "plt.axis([-6, 8, -8, 10])\n",
    "plt.axes().set_aspect('equal')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"$a_0$\")\n",
    "plt.ylabel(\"$a_1$\")\n",
    "plt.title(\"Back to $A$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Eigen-decomposition Approach\n",
    "1. Normalize columns of $A$ so that each feature has a mean of zero\n",
    "1. Compute sample covariance matrix $\\Sigma = {A^TA}/{(m-1)}$\n",
    "1. Perform eigen-decomposition of $\\Sigma$ using `np.linalg.eig(Sigma)`\n",
    "1. Compress by ordering $k$ evectors according to largest e-values and compute $AX_k$\n",
    "1. Reconstruct from the compressed version by computing $A X_k X_k^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above steps can be summarized with the following gif.\n",
    "[Wicked animated GIF which illustrates PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n",
    "\n",
    "Magically, eigen-decomposition (or PCA) finds the line where\n",
    "1. the spread of values along the black line is **maximal**\n",
    "2. the projection error (sum of red lines) is **minimal**\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/Q7HIP.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using the free glass datset.\n",
    "# The objective is to tell the type of glass based on amount of other elements present.\n",
    "data = pd.read_csv('glass.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(labels=['index','Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll go ahead and standardise this data as all the data is on a different scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaled_data=scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data=scaled_data, columns= data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(df)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above, it can be seen that 4 principal components explain almost 90% of the variance in data and 5 principal components explain around 95% of the variance in data.\n",
    "\n",
    "So, instead of giving all the columns as input, we’d only feed these 4 principal components of the data to the machine learning algorithm and we’d obtain a similar result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "new_data = pca.fit_transform(df)\n",
    "# This will be the new data fed to the algorithm.\n",
    "principal_Df = pd.DataFrame(data = new_data\n",
    "             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_Df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that earlier we had 9 columns in the data earlier. Now with the help of Scree plot and PCA, we have reduced the number of features to be used for model building to 4. This is the advantage of PCA. _It drastically reduces the number of features, thereby considerably reducing the training time for the model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Principal components\n",
    "\n",
    "As humans can only perceive 3dimensions, we’ll take a dataset with less than 4 dimensions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 200))).T\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA seeks to find the **Principal Axes** in the data, and explain how vital those axes are in describing the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see what these numbers mean, let's view them as vectors plotted on top of the data:\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.plot([0, v[0]], [0, v[1]], '-k', lw=3)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one vector is longer than the other. In a sense, this tells us that that direction in the data is somehow more \"important\" than the other direction.\n",
    "The explained variance quantifies this measure of \"importance\" in a direction.\n",
    "\n",
    "Another way to think of it is that the second principal component could be **completely ignored** without much loss of information! Let's see what our data look like if we only keep 95% of the variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PCA(0.95) # keep 95% of variance\n",
    "X_trans = clf.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(X_trans.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying that we want to throw away 5% of the variance, the data is now compressed by a factor of 50%! Let's see what the data look like after this compression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = clf.inverse_transform(X_trans)\n",
    "plt.plot(X[:, 0], X[:, 1], 'o', alpha=0.2)\n",
    "plt.plot(X_new[:, 0], X_new[:, 1], 'ob', alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lighter points are the original data, while the dark points are the projected version on the principal component axis.  We see that after truncating 5% of the variance of this dataset and then reprojecting it, the \"most important\" features of the data are maintained, and we've compressed the data by 50%!\n",
    "\n",
    "This is the sense in which \"dimensionality reduction\" works: if you can approximate a data set in a lower dimension, you can often have an easier time visualizing it or fitting complicated models to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of PCA to the Digits Data\n",
    "\n",
    "The dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data.  Let's implement PCA to the digits data. This data consists of a collection of different points in the plane to represent a digit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "Xproj = pca.fit_transform(X)\n",
    "print(X.shape)\n",
    "print(Xproj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a scatter plot of the datapoints\n",
    "plt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an idea of the relationship between the datapoints. Essentially, we have found the optimal stretch and rotation in 64-dimensional space and tried to fit it to a 2-Dimensional space that allows us to see the layout of the digits, **without reference** to the labels.\n",
    "\n",
    "### What do the Components Mean?\n",
    "\n",
    "This gives us an idea of the relationship between the datapoints. Essentially, we have made the data of 64 dimension fit to a 2-Dimensional space that allows us to see the layout of the digits, **without reference** to the labels.\n",
    "\n",
    "$$\n",
    "x = [x_1, x_2, x_3 \\cdots]\n",
    "$$\n",
    "\n",
    "but what this really means is\n",
    "\n",
    "$$\n",
    "image(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots\n",
    "$$\n",
    "\n",
    "If we reduce the dimensionality in the pixel space to (say) 6, we recover only a partial image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running this, download the fig_code library from Git\n",
    "from fig_code.figures import plot_image_components\n",
    "\n",
    "sns.set_style('white')\n",
    "plot_image_components(digits.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel-wise representation of those digits is not the only choice we have. We can also use other *basis functions*, and show it like:\n",
    "\n",
    "$$\n",
    "image(x) = {\\rm mean} + x_1 \\cdot{\\rm (basis~1)} + x_2 \\cdot{\\rm (basis~2)} + x_3 \\cdot{\\rm (basis~3)} \\cdots\n",
    "$$\n",
    "\n",
    "What PCA does is to choose optimal **basis functions** so that only a few are needed to get a reasonable approximation.\n",
    "The low-dimensional representation of our data is the coefficients of this series, and the approximate reconstruction is the result of the sum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fig_code.figures import plot_pca_interactive\n",
    "plot_pca_interactive(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that with only six PCA components, we recover a reasonable approximation of the input!\n",
    "\n",
    "Thus we see that PCA can be viewed from two angles. It can be viewed as **dimensionality reduction**, or it can be viewed as a form of **lossy data compression** where the loss favours noise. In this way, PCA can be used as a **filtering** mechanism as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Number of Components\n",
    "\n",
    "But how much information have we thrown away?  We can figure this out by looking at the **explained variance** as a function of the components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pca = PCA().fit(X)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Scree plot, it can be seen that 20 components are required to explain 90% of the variance which is still better than computing using all the 64 features. The explained variance threshold can be choosen based on the doamin and business requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA for data compression\n",
    "\n",
    "As mentioned, PCA can be used for a sort of data compression as well. Using a smaller value of ``n_components`` allows you to represent a higher dimensional point as a sum of just a few principal component vectors.\n",
    "\n",
    "Here's what a single digit looks like when you change the number of components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    pca = PCA(i + 1).fit(X)\n",
    "    im = pca.inverse_transform(pca.transform(X[20:21]))\n",
    "\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at this by using IPython's ``interact`` functionality to view the reconstruction of several images at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_digits(n_components):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 1, 1, frameon=False, xticks=[], yticks=[])\n",
    "    nside = 10\n",
    "    \n",
    "    pca = PCA(n_components).fit(X)\n",
    "    Xproj = pca.inverse_transform(pca.transform(X[:nside ** 2]))\n",
    "    Xproj = np.reshape(Xproj, (nside, nside, 8, 8))\n",
    "    total_var = pca.explained_variance_ratio_.sum()\n",
    "    \n",
    "    im = np.vstack([np.hstack([Xproj[i, j] for j in range(nside)])\n",
    "                    for i in range(nside)])\n",
    "    plt.imshow(im)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"n = {0}, variance = {1:.2f}\".format(n_components, total_var),\n",
    "                 size=18)\n",
    "    plt.clim(0, 16)\n",
    "    \n",
    "interact(plot_digits, n_components=range(1, 64), nside=range(1, 8)) # A in blue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram above, we can dynamically select the number of principal components and get to know the explained percentage of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros of PCA:**\n",
    "\n",
    "- Correlated features are removed.\n",
    "- Model training time is reduced.\n",
    "- Overfitting is reduced.\n",
    "- Helps in better visualizations\n",
    "- Ability to handle noise\n",
    "\n",
    "**Cons of PCA**\n",
    "- The resultant principal components are less interpretable than the original data\n",
    "- Can lead to information loss if the explained variance threshold is not considered appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "From all the explanations above, we can conclude that PCA is a very powerful technique for reducing the dimensions of the data, projecting the data from a higher dimension to a lower dimension, helps in data visualization, helps in data compression and most of all increases the model training speed drastically by decreasing the number of variables involved in computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
